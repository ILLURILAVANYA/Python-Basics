# -*- coding: utf-8 -*-
"""18-02-2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JtHNs7pslcN3toMdM-e5wJy1Qtmh6RW8
"""

import nltk
import spacy

text = "Hello! How are you? I am learning NLP. Welcome to U.S.A. "

nlp = spacy.load("en_core_web_sm")
doc = nlp(text)
spacy_tokens = [token.text for token in doc]


print("SpaCy Tokens:", spacy_tokens)

spacy_stopwords = [token.text for token in doc if not token.is_stop]
print("SpaCy Filtered Words:", spacy_stopwords)

lemmatized_words = [token.lemma_ for token in doc]

print("Lemmatized Words:", lemmatized_words)

import re

text = "Hello!! This is NLP 101. Visit https://example.com"
cleaned_text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove non-alphabetic characters
lower_text = cleaned_text.lower()

print("Cleaned Text:", lower_text)

for token in doc:
    print(f"{token.text} --> {token.pos_}")

for ent in doc.ents:
    print(f"{ent.text} --> {ent.label_}")

from gensim.models import Word2Vec
from gensim.test.utils import common_texts


model = Word2Vec(sentences=common_texts, vector_size=2, window=5, min_count=1, workers=4)

print("Vector for 'computer':", model.wv['computer'])

print("Most similar words to 'computer':", model.wv.most_similar('eps'))

import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from sklearn.decomposition import PCA


word_vectors = model.wv[model.wv.index_to_key]  # Get the word vectors
pca = PCA(n_components=2)  # Initialize PCA
result = pca.fit_transform(word_vectors)  # Fit and transform the word vectors


plt.figure(figsize=(10, 8))
plt.scatter(result[:, 0], result[:, 1])


words = list(model.wv.index_to_key)
for i, word in enumerate(words):
    plt.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=12)

plt.title("Word Embeddings Visualization")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.grid()
plt.show()